{"cells":[{"cell_type":"markdown","metadata":{"id":"MNBgGYg_lpVN"},"source":["# Assignment Module 2: Product Classification\n","\n","The goal of this assignment is to implement a neural network that classifies smartphone pictures of products found in grocery stores. The assignment will be divided into two parts: first, you will be asked to implement from scratch your own neural network for image classification; then, you will fine-tune a pretrained network provided by PyTorch.\n"]},{"cell_type":"markdown","metadata":{"id":"dVTQUJ4uYH1w"},"source":["## Preliminaries: the dataset\n","\n","The dataset you will be using contains natural images of products taken with a smartphone camera in different grocery stores:\n","\n","<p align=\"center\">\n","  <img src=\"https://github.com/marcusklasson/GroceryStoreDataset/raw/master/sample_images/natural/Granny-Smith.jpg\" width=\"150\">\n","  <img src=\"https://github.com/marcusklasson/GroceryStoreDataset/raw/master/sample_images/natural/Pink-Lady.jpg\" width=\"150\">\n","  <img src=\"https://github.com/marcusklasson/GroceryStoreDataset/raw/master/sample_images/natural/Lemon.jpg\" width=\"150\">\n","  <img src=\"https://github.com/marcusklasson/GroceryStoreDataset/raw/master/sample_images/natural/Banana.jpg\" width=\"150\">\n","  <img src=\"https://github.com/marcusklasson/GroceryStoreDataset/raw/master/sample_images/natural/Vine-Tomato.jpg\" width=\"150\">\n","</p>\n","<p align=\"center\">\n","  <img src=\"https://github.com/marcusklasson/GroceryStoreDataset/raw/master/sample_images/natural/Yellow-Onion.jpg\" width=\"150\">\n","  <img src=\"https://github.com/marcusklasson/GroceryStoreDataset/raw/master/sample_images/natural/Green-Bell-Pepper.jpg\" width=\"150\">\n","  <img src=\"https://github.com/marcusklasson/GroceryStoreDataset/raw/master/sample_images/natural/Arla-Standard-Milk.jpg\" width=\"150\">\n","  <img src=\"https://github.com/marcusklasson/GroceryStoreDataset/raw/master/sample_images/natural/Oatly-Natural-Oatghurt.jpg\" width=\"150\">\n","  <img src=\"https://github.com/marcusklasson/GroceryStoreDataset/raw/master/sample_images/natural/Alpro-Fresh-Soy-Milk.jpg\" width=\"150\">\n","</p>\n","\n","The products belong to the following 43 classes:\n","```\n","0.  Apple\n","1.  Avocado\n","2.  Banana\n","3.  Kiwi\n","4.  Lemon\n","5.  Lime\n","6.  Mango\n","7.  Melon\n","8.  Nectarine\n","9.  Orange\n","10. Papaya\n","11. Passion-Fruit\n","12. Peach\n","13. Pear\n","14. Pineapple\n","15. Plum\n","16. Pomegranate\n","17. Red-Grapefruit\n","18. Satsumas\n","19. Juice\n","20. Milk\n","21. Oatghurt\n","22. Oat-Milk\n","23. Sour-Cream\n","24. Sour-Milk\n","25. Soyghurt\n","26. Soy-Milk\n","27. Yoghurt\n","28. Asparagus\n","29. Aubergine\n","30. Cabbage\n","31. Carrots\n","32. Cucumber\n","33. Garlic\n","34. Ginger\n","35. Leek\n","36. Mushroom\n","37. Onion\n","38. Pepper\n","39. Potato\n","40. Red-Beet\n","41. Tomato\n","42. Zucchini\n","```\n","\n","The dataset is split into training (`train`), validation (`val`), and test (`test`) set."]},{"cell_type":"markdown","metadata":{"id":"1pdrmJRnJPd8"},"source":["The following code cells download the dataset and define a `torch.utils.data.Dataset` class to access it. This `Dataset` class will be the starting point of your assignment: use it in your own code and build everything else around it."]},{"cell_type":"code","execution_count":36,"metadata":{"id":"POMX_3x-_bZI"},"outputs":[{"name":"stdout","output_type":"stream","text":["fatal: destination path 'GroceryStoreDataset' already exists and is not an empty directory.\n"]}],"source":["!git clone https://github.com/marcusklasson/GroceryStoreDataset.git"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"hiF8xGEYlsu8"},"outputs":[],"source":["from pathlib import Path\n","from PIL import Image\n","from torch import Tensor\n","from torch.utils.data import Dataset\n","from typing import List, Tuple"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"jROSO2qVDxdD"},"outputs":[],"source":["class GroceryStoreDataset(Dataset):\n","\n","    def __init__(self, split: str, transform=None) -> None:\n","        super().__init__()\n","\n","        self.root = Path(\"GroceryStoreDataset/dataset\")\n","        self.split = split\n","        self.paths, self.labels = self.read_file()\n","\n","        self.transform = transform\n","\n","    def __len__(self) -> int:\n","        return len(self.labels)\n","\n","    def __getitem__(self, idx) -> Tuple[Tensor, int]:\n","        img = Image.open(self.root / self.paths[idx])\n","        label = self.labels[idx]\n","\n","        if self.transform:\n","            img = self.transform(img)\n","\n","        return img, label\n","\n","    def read_file(self) -> Tuple[List[str], List[int]]:\n","        paths = []\n","        labels = []\n","\n","        with open(self.root / f\"{self.split}.txt\") as f:\n","            for line in f:\n","                # path, fine-grained class, coarse-grained class\n","                path, _, label = line.replace(\"\\n\", \"\").split(\", \")\n","                paths.append(path), labels.append(int(label))\n","\n","        return paths, labels\n","\n","    def get_num_classes(self) -> int:\n","        return max(self.labels) + 1"]},{"cell_type":"markdown","metadata":{"id":"yBch3dpwNSsW"},"source":["## Part 1: design your own network\n","\n","Your goal is to implement a convolutional neural network for image classification and train it on `GroceryStoreDataset`. You should consider yourselves satisfied once you obtain a classification accuracy on the **validation** split of **around 60%**. You are free to achieve that however you want, except for a few rules you must follow:\n","\n","- You **cannot** simply instantiate an off-the-self PyTorch network. Instead, you must construct your network as a composition of existing PyTorch layers. In more concrete terms, you can use e.g. `torch.nn.Linear`, but you **cannot** use e.g. `torchvision.models.alexnet`.\n","\n","- Justify every *design choice* you make. Design choices include network architecture, training hyperparameters, and, possibly, dataset preprocessing steps. You can either (i) start from the simplest convolutional network you can think of and add complexity one step at a time, while showing how each step gets you closer to the target ~60%, or (ii) start from a model that is already able to achieve the desired accuracy and show how, by removing some of its components, its performance drops (i.e. an *ablation study*). You can *show* your results/improvements however you want: training plots, console-printed values or tables, or whatever else your heart desires: the clearer, the better.\n","\n","Don't be too concerned with your network performance: the ~60% is just to give you an idea of when to stop. Keep in mind that a thoroughly justified model with lower accuracy will be rewarded **more** points than a poorly experimentally validated model with higher accuracy."]},{"cell_type":"markdown","metadata":{"id":"gkWEqSPoUIL3"},"source":["## Part 2: fine-tune an existing network\n","\n","Your goal is to fine-tune a pretrained **ResNet-18** model on `GroceryStoreDataset`. Use the implementation provided by PyTorch, do not implement it yourselves! (i.e. exactly what you **could not** do in the first part of the assignment). Specifically, you must use the PyTorch ResNet-18 model pretrained on ImageNet-1K (V1). Divide your fine-tuning into two parts:\n","\n","1. First, fine-tune the Resnet-18 with the same training hyperparameters you used for your best model in the first part of the assignment.\n","1. Then, tweak the training hyperparameters in order to increase the accuracy on the validation split of `GroceryStoreDataset`. Justify your choices by analyzing the training plots and/or citing sources that guided you in your decisions (papers, blog posts, YouTube videos, or whatever else you find enlightening). You should consider yourselves satisfied once you obtain a classification accuracy on the **validation** split **between 80 and 90%**."]},{"cell_type":"markdown","metadata":{},"source":["---\n","---"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Device: cpu\n"]}],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torch.utils.data import Dataset\n","import numpy as np\n","import random\n","from typing import List, Tuple\n","from torch import Tensor\n","from pathlib import Path\n","from torchvision import transforms as T, datasets\n","from PIL import Image\n","from torch.utils.data import random_split, DataLoader\n","from tqdm import tqdm\n","\n","device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n","print(f\"Device: {device}\")\n","\n","def fix_random(seed: int) -> None:\n","    \"\"\"\n","        Fix all the possible sources of randomness.\n","    \"\"\"\n","    np.random.seed(seed)\n","    random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)\n","\n","    torch.backends.cudnn.benchmark = False\n","    torch.backends.cudnn.deterministic = True"]},{"cell_type":"markdown","metadata":{},"source":["## Data preprocessing"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[],"source":["import torchvision.transforms as T\n","from torch.utils.data import ConcatDataset\n","\n","# Define the data augmentation transformations\n","NUM_CLASSES = 43\n","INPUT_WIDTH = 348\n","INPUT_HEIGHT = 348\n","BATCH_SIZE = 64\n","USE_DATA_AUGMENTATION = True\n","AUGMENTATION_FACTOR = 2\n","\n","tsfms = T.Compose([\n","    T.ToTensor(),\n","    T.CenterCrop((INPUT_HEIGHT, INPUT_WIDTH)) # Some images are not the same size\n","])\n","augmentation_transforms = [T.Compose([\n","    T.RandomHorizontalFlip(),\n","    T.RandomRotation(10),\n","    T.RandomResizedCrop((INPUT_HEIGHT, INPUT_WIDTH), scale=(0.8, 1.0)),\n","    T.ToTensor(),\n","    T.CenterCrop((INPUT_HEIGHT, INPUT_WIDTH)) # Some images are not the same size\n","]), T.Compose([\n","    T.RandomHorizontalFlip(),\n","    T.RandomRotation(10),\n","    T.RandomResizedCrop((INPUT_HEIGHT, INPUT_WIDTH), scale=(0.8, 1.0)),\n","    T.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2),\n","    T.ToTensor(),\n","    T.CenterCrop((INPUT_HEIGHT, INPUT_WIDTH)) # Some images are not the same size\n","])]\n","\n","# Check if thge file 'train_dset.pt' exists\n","if Path('train_dset.pt').exists():\n","    train_dset = torch.load('train_dset.pt')\n","else:\n","    # Concatenate the original and augmented training datasets\n","    train_dset = GroceryStoreDataset(split=\"train\", transform=tsfms)\n","    if USE_DATA_AUGMENTATION:\n","        print(f'Train dataset size before augmentation: {len(train_dset)}')\n","        for i in range(AUGMENTATION_FACTOR):\n","            for j in range(len(augmentation_transforms)):\n","                augmented_train_dset = GroceryStoreDataset(split=\"train\", transform=augmentation_transforms[j])\n","                train_dset = ConcatDataset([train_dset, augmented_train_dset])\n","        print(f'Train dataset size after augmentation: {len(train_dset)}')\n","        # Dump the whole dataset for later use in order to avoid recomputing the augmentations\n","        torch.save(train_dset, 'train_dset.pt')\n","\n","\n","test_dset = GroceryStoreDataset(split=\"test\", transform=tsfms)\n","val_dset = GroceryStoreDataset(split=\"val\", transform=tsfms)\n","\n","val_dl = DataLoader(val_dset, 128)\n","test_dl = DataLoader(test_dset, 128)\n","train_dl = DataLoader(train_dset, BATCH_SIZE)"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["def evaluate(model, test_dl, device):\n","    corrects = 0\n","    total = 0\n","    model = model.to(device)\n","\n","    with torch.no_grad():\n","        model.eval()\n","        for image, labels in test_dl:\n","            image, labels = image.to(device), labels.to(device)\n","            pred_logits = model(image)\n","            _, preds = torch.max(pred_logits, 1)\n","            corrects += (preds == labels).sum().item()\n","            total += labels.shape[0]\n","\n","    return corrects / total"]},{"cell_type":"markdown","metadata":{},"source":["## Part 1"]},{"cell_type":"markdown","metadata":{},"source":["### Basic Model\n","\n","Each Convolutional block in the network has the following structure:\n","1. 2D Convolution\n","2. SiLU Activation Function\n","3. Max Pooling\n","4. Batch Normalization\n","5. Dropout"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["class ConvBlock(nn.Module):\n","    def __init__(self, kernel_size, in_channels, out_channels, activation):\n","        super().__init__()\n","        self.in_channels = in_channels\n","        self.out_channels = out_channels\n","        \n","        self.block = nn.Sequential(\n","            nn.Conv2d(\n","                in_channels = in_channels, \n","                out_channels = out_channels, \n","                kernel_size = kernel_size, \n","                stride = 2, \n","                padding = \"valid\"\n","            ),\n","            nn.BatchNorm2d(out_channels),\n","            activation,\n","            nn.Conv2d(\n","                in_channels = out_channels, \n","                out_channels = out_channels*2, \n","                kernel_size = kernel_size, \n","                stride = 1, \n","                padding = \"same\"\n","            ),\n","            nn.BatchNorm2d(out_channels*2),\n","            activation,\n","            nn.Conv2d(\n","                in_channels = out_channels*2, \n","                out_channels = out_channels, \n","                kernel_size = kernel_size, \n","                stride = 1, \n","                padding = \"same\"\n","            ),\n","            nn.BatchNorm2d(out_channels),\n","        )\n","        \n","        self.activation = activation\n","        \n","        self.skip_block = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=2, padding=\"valid\")\n","    \n","    def forward(self, x):\n","        # Residual connection\n","        residual = self.skip_block(x)\n","        x = self.block(x) + residual\n","        return self.activation(x)\n","        "]},{"cell_type":"markdown","metadata":{},"source":["### First Model"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[],"source":["class FirstModel(nn.Module):\n","    def __init__(self, input_dim, n_classes):\n","        super().__init__()\n","        \n","        self.stem_block = nn.Sequential(\n","            nn.Conv2d(in_channels=3, out_channels=64, kernel_size=7, stride=3, padding=\"valid\"),\n","            nn.SiLU(),\n","            nn.BatchNorm2d(64),\n","            nn.Dropout(p=0.05),\n","            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=5, stride=2, padding=\"valid\"),\n","            nn.SiLU(),\n","            nn.BatchNorm2d(128),\n","            nn.Dropout(p=0.05)\n","        )\n","                \n","        self.conv_block_1 = ConvBlock(kernel_size=3, in_channels=128, out_channels=256, activation=nn.SiLU())\n","        self.conv_block_2 = ConvBlock(kernel_size=3, in_channels=256, out_channels=256, activation=nn.SiLU())\n","        # self.conv_block_3 = ConvBlock(kernel_size=3, in_channels=256, out_channels=512, activation=nn.SiLU())\n","        \n","        self.linear_block = nn.Sequential(\n","            nn.Linear(256, 1024),\n","            nn.SiLU(),\n","            nn.Dropout(p=0.2),\n","            nn.Linear(1024, 2048),\n","            nn.SiLU(),\n","            nn.Dropout(p=0.2),\n","            #nn.Linear(2048, 2048),\n","            #nn.SiLU(),\n","            #nn.Dropout(p=0.2),\n","            nn.Linear(2048, n_classes)\n","        )        \n","        \n","    def forward(self, x):\n","        x = self.stem_block(x)\n","        \n","        x = self.conv_block_1(x)\n","        x = self.conv_block_2(x)\n","        # x = self.conv_block_3(x)\n","        \n","        # Before flattening the tensor, to further reduce the parameters\n","        # we use adaptive average pooling\n","        x = F.adaptive_avg_pool2d(x, (1, 1))\n","        x = torch.flatten(x, 1)\n","        \n","        x = self.linear_block(x)\n","        return x\n"]},{"cell_type":"code","execution_count":19,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([[-0.1004,  0.0555, -0.0185, -0.1042,  0.0835,  0.0452, -0.0722,  0.1008,\n","         -0.0017, -0.0011, -0.1373,  0.0230,  0.0372, -0.0906,  0.0155, -0.1794,\n","          0.0249,  0.0167, -0.1061,  0.0326, -0.0333,  0.0961, -0.0480,  0.0239,\n","          0.0674,  0.0110, -0.0440, -0.0131,  0.0354, -0.0658, -0.0722, -0.0324,\n","          0.0354,  0.0149,  0.0192,  0.0057, -0.0393, -0.0251,  0.0081,  0.0661,\n","          0.0285,  0.0266,  0.0835]], grad_fn=<AddmmBackward0>)\n"]},{"name":"stderr","output_type":"stream","text":["  0%|          | 0/100 [06:33<?, ?it/s]\n"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[1;32mIn[19], line 105\u001b[0m\n\u001b[0;32m    102\u001b[0m model \u001b[38;5;241m=\u001b[39m FirstModel((INPUT_HEIGHT, INPUT_WIDTH), NUM_CLASSES)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m    103\u001b[0m \u001b[38;5;28mprint\u001b[39m(model(train_dset[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)))\n\u001b[1;32m--> 105\u001b[0m logs \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    106\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    107\u001b[0m \u001b[43m    \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCrossEntropyLoss\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    108\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43moptim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mAdam\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3e-5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-5\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    109\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dl\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtrain_dl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m    110\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_dl\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mval_dl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    111\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    112\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    113\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrandom_seed\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m42\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    114\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcheckpoint_path\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./checkpoints\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    115\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogging_path\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./logs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[0;32m    116\u001b[0m \u001b[43m)\u001b[49m\n","Cell \u001b[1;32mIn[19], line 43\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, loss_fn, optimizer, train_dl, val_dl, device, n_epochs, checkpoint_path, logging_path, random_seed)\u001b[0m\n\u001b[0;32m     41\u001b[0m pred_logits \u001b[38;5;241m=\u001b[39m model(images)\n\u001b[0;32m     42\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(pred_logits, labels)\n\u001b[1;32m---> 43\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     44\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m device \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n","File \u001b[1;32mc:\\Users\\Luca\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\_tensor.py:525\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    517\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    518\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    523\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    524\u001b[0m     )\n\u001b[1;32m--> 525\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    527\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[1;32mc:\\Users\\Luca\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\autograd\\__init__.py:267\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    262\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    266\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 267\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[1;32mc:\\Users\\Luca\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\autograd\\graph.py:744\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    742\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    743\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    745\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    746\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    747\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n","\u001b[1;31mKeyboardInterrupt\u001b[0m: "]}],"source":["\n","import os\n","import json\n","\n","\n","def train(\n","    model, \n","    loss_fn, \n","    optimizer, \n","    train_dl, \n","    val_dl, \n","    device, \n","    n_epochs, \n","    checkpoint_path = None,\n","    logging_path = None,\n","    random_seed = 42\n","):\n","    fix_random(random_seed)\n","    if checkpoint_path is not None: os.makedirs(checkpoint_path, exist_ok=True)\n","    if logging_path is not None: os.makedirs(logging_path, exist_ok=True)\n","\n","    history = []\n","    model = model.to(device)\n","    loss_fn = loss_fn.to(device)\n","\n","\n","    for epoch in tqdm(range(n_epochs)):\n","        # Train\n","        model.train()\n","        batch_train_accuracies = []\n","        batch_train_losses = []\n","        batch_train_times = []\n","        for i, (images, labels) in enumerate(train_dl):\n","            if device == \"cuda\":\n","                start_time = torch.cuda.Event(enable_timing=True)\n","                end_time = torch.cuda.Event(enable_timing=True)\n","            images, labels = images.to(device), labels.to(device)\n","\n","            if device == \"cuda\":\n","                start_time.record()\n","            optimizer.zero_grad()\n","            pred_logits = model(images)\n","            loss = loss_fn(pred_logits, labels)\n","            loss.backward()\n","            optimizer.step()\n","            if device == \"cuda\":\n","                end_time.record()\n","            \n","            _, preds = torch.max(pred_logits, 1)\n","            batch_train_accuracies.append((preds == labels).sum().item() / labels.size(0))\n","            batch_train_losses.append(loss.item())\n","            if device == \"cuda\":\n","                torch.cuda.synchronize()\n","                batch_train_times.append(start_time.elapsed_time(end_time))\n","\n","\n","        # Validation\n","        batch_val_accuracy = 0\n","        batch_val_loss = 0\n","        \n","        model.eval()\n","        with torch.no_grad():\n","            for images, labels in val_dl:\n","                images, labels = images.to(device), labels.to(device)\n","                preds_logits = model(images)\n","                _, pred = torch.max(preds_logits, 1)\n","                batch_val_accuracy += (pred == labels).sum().item()\n","                batch_val_loss += loss_fn(preds_logits, labels).item() * labels.shape[0]\n","        batch_val_accuracy = batch_val_accuracy / len(val_dl.dataset)\n","        batch_val_loss = batch_val_loss / len(val_dl.dataset)\n","\n","        print(f\"Epoch {epoch}, \"\n","                f\"train_acc: {np.mean(batch_train_accuracies):.4f}, \"\n","                f\"val_acc: {batch_val_accuracy:.4f}, \"\n","                f\"train_loss: {np.mean(batch_train_losses):.4f}, \"\n","                f\"val_loss: {batch_val_loss:.4f}\")\n","        \n","        # Checkpoint\n","        if checkpoint_path is not None:\n","            torch.save({\n","                \"state_dict\": model.state_dict(),\n","                \"architecture\": str(model)\n","            }, os.path.join(checkpoint_path, f\"epoch{epoch+1}.ckp.pt\"))\n","\n","        # Logging\n","        epoch_logs = {\n","            \"epoch\": epoch + 1,\n","            \"batch_train_accuracies\": batch_train_accuracies,\n","            \"batch_train_losses\": batch_train_losses,\n","            \"batch_train_times\": batch_train_times,\n","            \"batch_val_accuracy\": batch_val_accuracy,\n","            \"batch_val_loss\": batch_val_loss\n","        }\n","        if logging_path is not None:\n","            with open(os.path.join(logging_path, f\"epoch{epoch+1}.log.json\"), \"w\") as f:\n","                json.dump(epoch_logs, f, indent=3)\n","        history.append(epoch_logs)\n","\n","        # print(\"Test acc: \", evaluate(model, test_dl, device))\n","            \n","    return history\n","\n","model = FirstModel((INPUT_HEIGHT, INPUT_WIDTH), NUM_CLASSES).to(device)\n","print(model(train_dset[0][0].unsqueeze(0).to(device)))\n","\n","logs = train(\n","    model = model,\n","    loss_fn = nn.CrossEntropyLoss(),\n","    optimizer = optim.Adam(model.parameters(), lr=3e-5, weight_decay=1e-5),\n","    train_dl = train_dl, \n","    val_dl = val_dl,\n","    device = device,\n","    n_epochs = 100,\n","    random_seed = 42,\n","    checkpoint_path = \"./checkpoints\",\n","    logging_path = \"./logs\"\n",")\n"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[],"source":["def print_model_parameters(model):\n","    total_params = 0\n","    for name, parameter in model.named_parameters():\n","        if not parameter.requires_grad: continue\n","        param_count = parameter.numel()\n","        total_params += param_count\n","    \n","    for name, parameter in model.named_parameters():\n","        if not parameter.requires_grad: continue\n","        param_count = parameter.numel()\n","        \n","        print(f\"Layer: {name} | Parameters: {param_count/total_params*100:.2f}%\")\n","    print(f\"Total Trainable Parameters: {total_params}\")"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Layer: stem_block.0.weight | Parameters: 0.10%\n","Layer: stem_block.0.bias | Parameters: 0.00%\n","Layer: stem_block.2.weight | Parameters: 0.00%\n","Layer: stem_block.2.bias | Parameters: 0.00%\n","Layer: stem_block.4.weight | Parameters: 2.24%\n","Layer: stem_block.4.bias | Parameters: 0.00%\n","Layer: stem_block.6.weight | Parameters: 0.00%\n","Layer: stem_block.6.bias | Parameters: 0.00%\n","Layer: conv_block_1.block.0.weight | Parameters: 3.22%\n","Layer: conv_block_1.block.0.bias | Parameters: 0.00%\n","Layer: conv_block_1.block.1.weight | Parameters: 0.00%\n","Layer: conv_block_1.block.1.bias | Parameters: 0.00%\n","Layer: conv_block_1.block.3.weight | Parameters: 12.88%\n","Layer: conv_block_1.block.3.bias | Parameters: 0.01%\n","Layer: conv_block_1.block.4.weight | Parameters: 0.01%\n","Layer: conv_block_1.block.4.bias | Parameters: 0.01%\n","Layer: conv_block_1.block.6.weight | Parameters: 12.88%\n","Layer: conv_block_1.block.6.bias | Parameters: 0.00%\n","Layer: conv_block_1.block.7.weight | Parameters: 0.00%\n","Layer: conv_block_1.block.7.bias | Parameters: 0.00%\n","Layer: conv_block_1.skip_block.weight | Parameters: 3.22%\n","Layer: conv_block_1.skip_block.bias | Parameters: 0.00%\n","Layer: conv_block_2.block.0.weight | Parameters: 6.44%\n","Layer: conv_block_2.block.0.bias | Parameters: 0.00%\n","Layer: conv_block_2.block.1.weight | Parameters: 0.00%\n","Layer: conv_block_2.block.1.bias | Parameters: 0.00%\n","Layer: conv_block_2.block.3.weight | Parameters: 12.88%\n","Layer: conv_block_2.block.3.bias | Parameters: 0.01%\n","Layer: conv_block_2.block.4.weight | Parameters: 0.01%\n","Layer: conv_block_2.block.4.bias | Parameters: 0.01%\n","Layer: conv_block_2.block.6.weight | Parameters: 12.88%\n","Layer: conv_block_2.block.6.bias | Parameters: 0.00%\n","Layer: conv_block_2.block.7.weight | Parameters: 0.00%\n","Layer: conv_block_2.block.7.bias | Parameters: 0.00%\n","Layer: conv_block_2.skip_block.weight | Parameters: 6.44%\n","Layer: conv_block_2.skip_block.bias | Parameters: 0.00%\n","Layer: linear_block.0.weight | Parameters: 2.86%\n","Layer: linear_block.0.bias | Parameters: 0.01%\n","Layer: linear_block.3.weight | Parameters: 22.89%\n","Layer: linear_block.3.bias | Parameters: 0.02%\n","Layer: linear_block.6.weight | Parameters: 0.96%\n","Layer: linear_block.6.bias | Parameters: 0.00%\n","Total Trainable Parameters: 9159979\n"]}],"source":["model = FirstModel((INPUT_HEIGHT, INPUT_WIDTH), NUM_CLASSES).to(device)\n","print_model_parameters(model)"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["----------------------------------------------------------------\n","        Layer (type)               Output Shape         Param #\n","================================================================\n","            Conv2d-1         [-1, 64, 114, 114]           9,472\n","              SiLU-2         [-1, 64, 114, 114]               0\n","       BatchNorm2d-3         [-1, 64, 114, 114]             128\n","           Dropout-4         [-1, 64, 114, 114]               0\n","            Conv2d-5          [-1, 128, 55, 55]         204,928\n","              SiLU-6          [-1, 128, 55, 55]               0\n","       BatchNorm2d-7          [-1, 128, 55, 55]             256\n","           Dropout-8          [-1, 128, 55, 55]               0\n","            Conv2d-9          [-1, 256, 27, 27]         295,168\n","           Conv2d-10          [-1, 256, 27, 27]         295,168\n","      BatchNorm2d-11          [-1, 256, 27, 27]             512\n","             SiLU-12          [-1, 256, 27, 27]               0\n","             SiLU-13          [-1, 256, 27, 27]               0\n","           Conv2d-14          [-1, 512, 27, 27]       1,180,160\n","      BatchNorm2d-15          [-1, 512, 27, 27]           1,024\n","             SiLU-16          [-1, 512, 27, 27]               0\n","             SiLU-17          [-1, 512, 27, 27]               0\n","           Conv2d-18          [-1, 256, 27, 27]       1,179,904\n","      BatchNorm2d-19          [-1, 256, 27, 27]             512\n","             SiLU-20          [-1, 256, 27, 27]               0\n","             SiLU-21          [-1, 256, 27, 27]               0\n","        ConvBlock-22          [-1, 256, 27, 27]               0\n","           Conv2d-23          [-1, 256, 13, 13]         590,080\n","           Conv2d-24          [-1, 256, 13, 13]         590,080\n","      BatchNorm2d-25          [-1, 256, 13, 13]             512\n","             SiLU-26          [-1, 256, 13, 13]               0\n","             SiLU-27          [-1, 256, 13, 13]               0\n","           Conv2d-28          [-1, 512, 13, 13]       1,180,160\n","      BatchNorm2d-29          [-1, 512, 13, 13]           1,024\n","             SiLU-30          [-1, 512, 13, 13]               0\n","             SiLU-31          [-1, 512, 13, 13]               0\n","           Conv2d-32          [-1, 256, 13, 13]       1,179,904\n","      BatchNorm2d-33          [-1, 256, 13, 13]             512\n","             SiLU-34          [-1, 256, 13, 13]               0\n","             SiLU-35          [-1, 256, 13, 13]               0\n","        ConvBlock-36          [-1, 256, 13, 13]               0\n","           Linear-37                 [-1, 1024]         263,168\n","             SiLU-38                 [-1, 1024]               0\n","          Dropout-39                 [-1, 1024]               0\n","           Linear-40                 [-1, 2048]       2,099,200\n","             SiLU-41                 [-1, 2048]               0\n","          Dropout-42                 [-1, 2048]               0\n","           Linear-43                   [-1, 43]          88,107\n","================================================================\n","Total params: 9,159,979\n","Trainable params: 9,159,979\n","Non-trainable params: 0\n","----------------------------------------------------------------\n","Input size (MB): 1.39\n","Forward/backward pass size (MB): 68.84\n","Params size (MB): 34.94\n","Estimated Total Size (MB): 105.17\n","----------------------------------------------------------------\n"]}],"source":["from torchsummary import summary\n","\n","summary(model, (3 , INPUT_HEIGHT, INPUT_WIDTH), device=\"cpu\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"colab":{"gpuType":"T4","provenance":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.11"}},"nbformat":4,"nbformat_minor":0}
